{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dc5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LogisticRegression:\n",
    "    \n",
    "#     def __init__(self, k, n, method, alpha = 0.001, max_iter=5000):\n",
    "#         self.k = k\n",
    "#         self.n = n\n",
    "#         self.alpha = alpha\n",
    "#         self.max_iter = max_iter\n",
    "#         self.method = method\n",
    "    \n",
    "#     def fit(self, X, Y):\n",
    "#         self.W = np.random.rand(self.n, self.k) \n",
    "#         self.losses = []\n",
    "        \n",
    "#         if self.method == \"batch\":\n",
    "#             start_time = time.time()\n",
    "#             for i in range(self.max_iter):\n",
    "#                 loss, grad =  self.gradient(X, Y)\n",
    "#                 self.losses.append(loss)\n",
    "#                 self.W = self.W - self.alpha * grad\n",
    "#                 if i % 100 == 0:\n",
    "#                     print(f\"Loss at iteration {i}\", loss)\n",
    "#             print(f\"time taken: {time.time() - start_time}\")\n",
    "            \n",
    "#         elif self.method == \"minibatch\":\n",
    "#             start_time = time.time()\n",
    "#             batch_size = int(0.3 * X.shape[0])\n",
    "#             for i in range(self.max_iter):\n",
    "#                 ix = np.random.randint(0, X.shape[0]) #with replacement\n",
    "#                 batch_X = X[ix:ix+batch_size]\n",
    "#                 batch_Y = Y[ix:ix+batch_size]\n",
    "#                 loss, grad = self.gradient(batch_X, batch_Y)\n",
    "#                 self.losses.append(loss)\n",
    "#                 self.W = self.W - self.alpha * grad\n",
    "#                 if i % 100 == 0:\n",
    "#                     print(f\"Loss at iteration {i}\", loss)\n",
    "#             print(f\"time taken: {time.time() - start_time}\")\n",
    "            \n",
    "#         elif self.method == \"sto\":\n",
    "#             start_time = time.time()\n",
    "#             list_of_used_ix = []\n",
    "#             for i in range(self.max_iter):\n",
    "#                 idx = np.random.randint(X.shape[0])\n",
    "#                 while i in list_of_used_ix:\n",
    "#                     idx = np.random.randint(X.shape[0])\n",
    "#                 X_train = X[idx, :].reshape(1, -1)\n",
    "#                 Y_train = Y[idx]\n",
    "#                 loss, grad = self.gradient(X_train, Y_train)\n",
    "#                 self.losses.append(loss)\n",
    "#                 self.W = self.W - self.alpha * grad\n",
    "                \n",
    "#                 list_of_used_ix.append(i)\n",
    "#                 if len(list_of_used_ix) == X.shape[0]:\n",
    "#                     list_of_used_ix = []\n",
    "#                 if i % 100 == 0:\n",
    "#                     print(f\"Loss at iteration {i}\", loss)\n",
    "#             print(f\"time taken: {time.time() - start_time}\")\n",
    "            \n",
    "#         else:\n",
    "#             raise ValueError('Method must be one of the followings: \"batch\", \"minibatch\" or \"sto\".')\n",
    "        \n",
    "\n",
    "#     def gradient(self, X, Y, lambda_=0):\n",
    "#         m = X.shape[0]\n",
    "#         h = self.h_theta(X, self.W)\n",
    "#         loss = - np.sum(Y * np.log(h + 1e-8)) / m + (lambda_ / (2 * m)) * np.sum(self.W**2)\n",
    "#         error = h - Y\n",
    "#         grad = (X.T @ error) / m + (lambda_ / m) * self.W\n",
    "#         return loss, grad\n",
    "\n",
    "\n",
    "#     # def softmax(self, theta_t_x):\n",
    "#     #     return np.exp(theta_t_x) / np.sum(np.exp(theta_t_x), axis=1, keepdims=True)\n",
    "\n",
    "#     def softmax(self, theta_t_x):\n",
    "#         # theta_t_x shape: (m, k)\n",
    "#         exp_vals = np.exp(theta_t_x - np.max(theta_t_x, axis=1, keepdims=True))\n",
    "#         return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
    "\n",
    "#     def softmax_grad(self, X, error):\n",
    "#         return  X.T @ error\n",
    "\n",
    "#     def h_theta(self, X, W):\n",
    "#         '''\n",
    "#         Input:\n",
    "#             X shape: (m, n)\n",
    "#             w shape: (n, k)\n",
    "#         Returns:\n",
    "#             yhat shape: (m, k)\n",
    "#         '''\n",
    "#         return self.softmax(X @ W)\n",
    "    \n",
    "#     def predict(self, X_test):\n",
    "#         return np.argmax(self.h_theta(X_test, self.W), axis=1)\n",
    "    \n",
    "#     def predict_proba(self, X_test):\n",
    "#         return self.h_theta(X_test, self.W)\n",
    "    \n",
    "#     def accuracy(self, y_true, y_pred):\n",
    "#         return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "#     def precision(self, y_true, y_pred, class_label):\n",
    "#         TP = np.sum((y_true == class_label) & (y_pred == class_label))\n",
    "#         FP = np.sum((y_true != class_label) & (y_pred == class_label))\n",
    "#         return TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "#     def recall(self, y_true, y_pred, class_label):\n",
    "#         TP = np.sum((y_true == class_label) & (y_pred == class_label))\n",
    "#         FN = np.sum((y_true == class_label) & (y_pred != class_label))\n",
    "#         return TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "#     def f1_score(self, y_true, y_pred, class_label):\n",
    "#         p = self.precision(y_true, y_pred, class_label)\n",
    "#         r = self.recall(y_true, y_pred, class_label)\n",
    "#         return 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    \n",
    "#     def macro_average(self, y_true, y_pred):\n",
    "#         classes = np.unique(y_true)\n",
    "#         precisions = [self.precision(y_true, y_pred, c) for c in classes]\n",
    "#         recalls = [self.recall(y_true, y_pred, c) for c in classes]\n",
    "#         f1s = [self.f1_score(y_true, y_pred, c) for c in classes]\n",
    "#         return np.mean(precisions), np.mean(recalls), np.mean(f1s)\n",
    "\n",
    "#     def weighted_average(self, y_true, y_pred):\n",
    "#         classes = np.unique(y_true)\n",
    "#         weights = [np.sum(y_true == c)/len(y_true) for c in classes]\n",
    "#         precisions = [self.precision(y_true, y_pred, c) for c in classes]\n",
    "#         recalls = [self.recall(y_true, y_pred, c) for c in classes]\n",
    "#         f1s = [self.f1_score(y_true, y_pred, c) for c in classes]\n",
    "#         weighted_p = np.sum([w*p for w, p in zip(weights, precisions)])\n",
    "#         weighted_r = np.sum([w*r for w, r in zip(weights, recalls)])\n",
    "#         weighted_f1 = np.sum([w*f for w, f in zip(weights, f1s)])\n",
    "#         return weighted_p, weighted_r, weighted_f1\n",
    "\n",
    "    \n",
    "#     def plot(self):\n",
    "#         plt.plot(np.arange(len(self.losses)) , self.losses, label = \"Train Losses\")\n",
    "#         plt.title(\"Losses\")\n",
    "#         plt.xlabel(\"epoch\")\n",
    "#         plt.ylabel(\"losses\")\n",
    "#         plt.legend()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
